{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4908f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from mpl_toolkits import mplot3d\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc09ab7",
   "metadata": {},
   "source": [
    "### Rosenbrock functions ( Jacobian and Hessian), error function, linear research functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell before launching the other algorithms\n",
    "\n",
    "\n",
    "def rosen(z):\n",
    "    x,y=z\n",
    "    return  (x-1)**2 + 100*(x**2-y)**2\n",
    "                   \n",
    "def diff_rosen(z):\n",
    "    x,y=z\n",
    "    return np.array([-400*x*(y-x**2)-2*(1-x),\n",
    "       200*(y-x**2)])\n",
    "        \n",
    "\n",
    "def hess_rosen(z):\n",
    "    x,y=z\n",
    "    return np.array([\n",
    "        [400*(-y+3*x**2)+2,  -400*x],\n",
    "        [-400*x,    200]\n",
    "    ])\n",
    "\n",
    "\n",
    "#Error function\n",
    "\n",
    "def compute_error(x,error):\n",
    "    # We will measure the distance with the target point (1,1) at each iteration\n",
    "    x_obj=np.array([1,1])\n",
    "    return np.linalg.norm(np.array(x)-x_obj)<error\n",
    "\n",
    "\n",
    "### GSS and Wolfe method\n",
    "\n",
    "def section_doree(f,a,b,nmax):\n",
    "    for i in range(nmax):\n",
    "        to=(1+np.sqrt(5))/2\n",
    "        a_temp=a+((b-a)/to**2)\n",
    "        b_temp=a+((b-a)/to)\n",
    "        if f(a_temp)<f(b_temp):\n",
    "            a_1=a\n",
    "            b_1=b_temp\n",
    "        if  f(a_temp)>f(b_temp):\n",
    "            a_1=a_temp\n",
    "            b_1=b\n",
    "        if  f(a_temp)==f(b_temp):\n",
    "            a_1=a_temp\n",
    "            b_1=b_temp\n",
    "      \n",
    "        a=a_1\n",
    "        b=b_1\n",
    "    return (a+b)/2\n",
    "\n",
    "def wolfe(f,df,x,c1=1e-4, c2=0.9, max_iters=50):\n",
    "    lr,lr_min,lr_max=1,0,0\n",
    "    grad=df(x)\n",
    "    d=-grad\n",
    "    for i in range(max_iters):\n",
    "        if (f(x + lr *d) <= f(x) + c1*lr*np.dot(grad,d)) and (f(x + lr *d) >= c2*np.dot(grad, d)):\n",
    "            return lr\n",
    "        else:\n",
    "            if (f(x + lr *d) > f(x) + c1*lr*np.dot(grad,d)):\n",
    "                lr_max=lr\n",
    "            elif (f(x + lr *d) < c2*np.dot(grad, d)):\n",
    "                lr_min=lr\n",
    "        if lr_max==0:\n",
    "            lr=2*lr_min\n",
    "        else:\n",
    "            lr=(lr_max-lr_min)/2\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af4c85",
   "metadata": {},
   "source": [
    "### Vizualisation of the Rosenbrock function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460dd95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = np.arange(-2,2.2,0.01), np.arange(-2,2.2,0.01)\n",
    "X,Y=np.meshgrid(x,y)\n",
    "z = rosen([X,Y])\n",
    "\n",
    "surface_trace=go.Surface(z=z, x=x, y=y)\n",
    "\n",
    "fig = go.Figure(data=[surface_trace])\n",
    "fig.update_layout(title=' The Rosenbrock function',title_x=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035dbae3",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fb72f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def newton2d(f,diff_rosen,z,epsilon=0.0000001):    \n",
    "    nb_iteration=0\n",
    "    liste_itere=[z]\n",
    "    while True:        \n",
    "        x,y = z        \n",
    "        inv_H=np.linalg.inv(diff_rosen(z))\n",
    "        x1,y1= np.array([x,y])-inv_H@f(z)\n",
    "        liste_itere.append((x1,y1))\n",
    "        z=x1,y1\n",
    "        nb_iteration+=1\n",
    "        \n",
    "        if compute_error(z,epsilon):\n",
    "            break          \n",
    "    return liste_itere,z,nb_iteration \n",
    "\n",
    "init=[-1.9,2]\n",
    "epsilon=0.0000001\n",
    "start=time.time()\n",
    "itéré,result,nb_iterations=newton2d(diff_rosen,hess_rosen,init)\n",
    "end=time.time()\n",
    "\n",
    "print(f\"nb_iterations: {nb_iterations}, result: {result}\\n\")\n",
    "print(f\"{end-start} s needed to converge to a solution\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64229fb",
   "metadata": {},
   "source": [
    "### Gradient Descent with fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad17465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradfixe(f,df,x,lr,epsilon,max_epochs):\n",
    "    \n",
    "    i=0\n",
    "    while not compute_error(x,epsilon) and i<max_epochs:\n",
    "        gradient=-df(x)\n",
    "        x1=x+lr*gradient\n",
    "        x=x1\n",
    "        i+=1\n",
    "    if i==max_epochs:\n",
    "        print(\"maximum number of epoch reached, epsilon= \",np.linalg.norm(x-np.array([1,1])))\n",
    "    return x1,i\n",
    "\n",
    "\n",
    "x=np.array([-1.9,2])\n",
    "error=0.0000001\n",
    "lr=0.001\n",
    "max_epochs=100000\n",
    "\n",
    "start=time.time()\n",
    "result,nb_iterations=gradfixe(rosen,diff_rosen,x,lr,error,max_epochs)\n",
    "end=time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nnb_iterations: {nb_iterations}, result: {result}\\n\")\n",
    "\n",
    "print(f\"{end-start} s needed to converge to a solution\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6f3ae",
   "metadata": {},
   "source": [
    "### Gradient descent with adaptative learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671f542",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gradopt(f,df,x,error,max_epochs,search_method):\n",
    "    liste_iter=[x]\n",
    "    i=0   \n",
    "    while not compute_error(x,error) and i<max_epochs:\n",
    "        gradient=df(x)\n",
    "        if search_method==\"section_doree\":\n",
    "            lr=section_doree(lambda lr:f(x-lr*gradient),0,1,50)\n",
    "        elif search_method==\"wolfe\":\n",
    "            lr=wolfe(f,df,x,0.3,0.7,1000)\n",
    "        else:\n",
    "            print(f\"{seach_method} is not recognized\")\n",
    "            break\n",
    "        x1=x-lr*gradient\n",
    "        epsilon=np.linalg.norm(x-x1,2)\n",
    "        i+=1\n",
    "        x=x1\n",
    "        liste_iter.append(x)\n",
    "    if i==max_epochs:\n",
    "        print(\"\\nmaximum number of epoch reached, epsilon= \",np.linalg.norm(x-np.array([1,1])))\n",
    "    return x1,liste_iter,i\n",
    "\n",
    "x_init=np.array([-1.9,2])\n",
    "error=0.0000001\n",
    "max_epochs=100000\n",
    "seach_method=\"wolfe\" #either \"section_doree\" or \"wolfe\"\n",
    "\n",
    "start=time.time()\n",
    "result,liste_iter,nb_iterations=gradopt(rosen,diff_rosen,x_init,error,max_epochs,seach_method)\n",
    "end=time.time()\n",
    "\n",
    "print(f\"\\nnb_iterations: {nb_iterations}, result: {result}\\n\")\n",
    "print(f\"{end-start} s needed to converge to a solution\") \n",
    "\n",
    "pts_x=np.array([liste_iter[i][0] for i in range(len(liste_iter))])\n",
    "pts_y=np.array([liste_iter[i][1] for i in range(len(liste_iter))])\n",
    "x, y = np.arange(-3,3,0.01), np.arange(-2,2.2,0.01)\n",
    "X,Y=np.meshgrid(x,y)\n",
    "z = rosen([X,Y])\n",
    "line_trace = go.Scatter3d(x=pts_x,y=pts_y,z=rosen([pts_x,pts_y]), mode='lines', line=dict(color='red', width=4))\n",
    "surface_trace=go.Surface(z=z, x=x, y=y)\n",
    "fig = go.Figure(data=[surface_trace,line_trace])\n",
    "fig.update_layout(title=' The Rosenbrock function and the steps achieved by the gradient descent before converging',title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53507ff4",
   "metadata": {},
   "source": [
    "### BFGS/DFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989dca44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bfgs(f,df, x, max_iterations=1000, tolerance=1e-6,methos=\"BFGS\"):\n",
    "    B = np.eye(len(x))\n",
    "    grad=df(x)   \n",
    "    p=-grad\n",
    "    liste_iter=[x]\n",
    "    for i in range(max_iterations):\n",
    "        lr=wolfe(f,df,x,0.3,0.7,50)\n",
    "        x_old=x\n",
    "        x=x_old+lr*p\n",
    "        liste_iter.append(x)   \n",
    "        if compute_error(x,tolerance):\n",
    "            break\n",
    "        grad_old=grad\n",
    "        grad=df(x)\n",
    "        s=x-x_old\n",
    "        y=grad-grad_old\n",
    "        y=y.reshape(2,1)\n",
    "        s=s.reshape(2,1)\n",
    "        r = 1/(y.T@s)\n",
    "        #We try to approximate directly the inverse of the Hessian in both BFGS and DFP\n",
    "        if method==\"BFGS\":\n",
    "            li = np.eye(2)-(r*(s@y.T))\n",
    "            ri = np.eye(2)-(r*(y@s.T))\n",
    "            hess_inter = li@B@ri\n",
    "            B = hess_inter + (r*(s@s.T))    \n",
    "        elif method==\"DFP\":     \n",
    "            B=B+(s@s.T/(s.T@y))-(B@y@y.T@B/(y.T@B@y))   \n",
    "        else:\n",
    "            print(f\"{method} is not recognized\")\n",
    "            break\n",
    "        p = -np.dot(B, grad)\n",
    "    if i==max_iterations-1:\n",
    "        print(\"\\nmaximum number of epoch reached, epsilon= \",np.linalg.norm(x-np.array([1,1])))\n",
    "    return x,liste_iter,i\n",
    "\n",
    "x_init=np.array([-1.9,2])\n",
    "error=0.0000001\n",
    "max_iter=1000000\n",
    "method=\"DFP\" # either \"BFGS\" or \"DFP\"\n",
    "\n",
    "start=time.time()\n",
    "result,liste_iter,nb_iterations=bfgs(rosen,diff_rosen,x_init,max_iter,error,method)\n",
    "end=time.time()\n",
    "\n",
    "print(f\"\\nnb_iterations: {nb_iterations}, result: {result}\\n\")\n",
    "\n",
    "print(f\"{end-start} s needed to converge to a solution\") \n",
    "\n",
    "pts_x=np.array([liste_iter[i][0] for i in range(len(liste_iter))])\n",
    "pts_y=np.array([liste_iter[i][1] for i in range(len(liste_iter))])\n",
    "x, y = np.arange(-3,3,0.01), np.arange(-2,2.2,0.01)\n",
    "X,Y=np.meshgrid(x,y)\n",
    "z = rosen([X,Y])\n",
    "\n",
    "line_trace = go.Scatter3d(x=pts_x,y=pts_y,z=rosen([pts_x,pts_y]), mode='lines', line=dict(color='red', width=4))\n",
    "surface_trace=go.Surface(z=z, x=x, y=y)\n",
    "\n",
    "fig = go.Figure(data=[surface_trace,line_trace])\n",
    "fig.update_layout(title=f' The Rosenbrock function and the steps achieved using {method} before converging',title_x=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925d799",
   "metadata": {},
   "source": [
    "## Conjugate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fcbc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating H and b\n",
    "def create_matrixes(n):\n",
    "    matrix = np.zeros((n, n)) \n",
    "    np.fill_diagonal(matrix, 4)\n",
    "    for i in range(n-1):\n",
    "        matrix[i, i+1] = -2\n",
    "        matrix[i+1, i] = -2\n",
    "        \n",
    "    return matrix,np.ones(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjGrad(H,x,b,tolerance,N): #finding x so that Hx=b\n",
    "    \n",
    "    r = b - H@x\n",
    "    \n",
    "    p = r.copy()\n",
    "    for i in range(N):\n",
    "        Hp = H@p\n",
    "        alpha = p@r/(p@Hp)\n",
    "        x = x + alpha*p\n",
    "        r = b - H@x\n",
    "        if np.sqrt(np.sum(r**2)) < tolerance:\n",
    "            break\n",
    "        else:\n",
    "            beta = -r@Hp/(p@Hp)\n",
    "            p = r + beta*p\n",
    "    return x \n",
    "\n",
    "n=100\n",
    "x=5*np.random.rand(n) #random x\n",
    "error=0.0000001\n",
    "\n",
    "start=time.time()\n",
    "H,b=create_matrixes(n)\n",
    "end=time.time()\n",
    "\n",
    "result=list(np.around(conjGrad(H,x,b,error,n),5))\n",
    "\n",
    "print(f\"Convergence achieved. Results:{result}\")\n",
    "\n",
    "print(f\"{end-start} s needed to converge to the solution\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
