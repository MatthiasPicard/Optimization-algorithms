# Optimization-algorithms
Notebook comparing several optimization algorithms to find the minimum of the Rosenbrock function: $$f(x,y)=(1-x)^2+100(y-x^2)^2$$
The following algorithms are implemented:
- Newton's method
- Gradient descent (fixed learning rate)
- Gradient descent (adaptative learning rate, using line search methods)
- Golden section search 
- Wolfe method
- BFGS
- DFP
- Conjugate Gradient method
